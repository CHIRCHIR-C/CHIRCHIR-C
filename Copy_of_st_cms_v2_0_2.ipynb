{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHIRCHIR-C/CHIRCHIR-C/blob/main/Copy_of_st_cms_v2_0_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kewXNlQ4NUC_",
        "outputId": "fd0053bb-430b-4c19-b55d-da7273a5f54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Installing necessary packages...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# --- COLAB SETUP: Mount Drive and Install Packages ---\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "# This is required to access your data and save results persistently.\n",
        "# Run this cell, follow the prompt, and select your Google account.\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Install Required Libraries\n",
        "# We consolidate all 'pip install' commands here using '!pip'.\n",
        "# PyMuPDF (fitz) and PyPDF2 are necessary for the script.\n",
        "print(\"Installing necessary packages...\")\n",
        "!pip install --quiet PyMuPDF PyPDF2 pandas\n",
        "\n",
        "# Note: The 'fitz' import is available after installing PyMuPDF\n",
        "# The 'PyPDF2' import is available after installing PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zz31-z5TLCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ac2c4f-ea65-4e54-94f3-05c6184a5526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the name of the subfolder to merge the PDFs (e.g., '07'): 09\n",
            "Enter the Period (YYYYMM, e.g., 202507): 202509\n",
            "Merging 8 PDF files...\n",
            "\n",
            "✅ Merged file 'st_202509_STITCHED.pdf' successfully created in /content/drive/MyDrive/Vanguard Analytics/General/Assignments/The Star/Publications/Merged\n"
          ]
        }
      ],
      "source": [
        "# --- PDF MERGER LOGIC (Adapted for Google Drive) ---\n",
        "import fitz\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from PyPDF2 import PdfMerger\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Replace 'Your_Drive_Folder' with the actual path in your Google Drive.\n",
        "# Example: '/content/drive/MyDrive/Publications/2025'\n",
        "# Base directory where the subfolders (like '07') are located\n",
        "BASE_DRIVE_FOLDER = '/content/drive/MyDrive/Vanguard Analytics/General/Assignments/The Star/Publications/2025'\n",
        "base_path = Path(BASE_DRIVE_FOLDER)\n",
        "\n",
        "# Define the persistent output directory for the merged PDF\n",
        "# Example: '/content/drive/MyDrive/Reports/Merged_Dailies'\n",
        "OUTPUT_DRIVE_FOLDER = '/content/drive/MyDrive/Vanguard Analytics/General/Assignments/The Star/Publications/Merged'\n",
        "output_dir = Path(OUTPUT_DRIVE_FOLDER)\n",
        "output_dir.mkdir(parents=True, exist_ok=True) # Ensure output directory exists\n",
        "\n",
        "# Prompt user for the subfolder name\n",
        "subfolder_name = input(\"Enter the name of the subfolder to merge the PDFs (e.g., '07'): \")\n",
        "subfolder_path = base_path / subfolder_name\n",
        "\n",
        "# Check if the subfolder exists\n",
        "if not subfolder_path.exists() or not subfolder_path.is_dir():\n",
        "    print(f\"Error: The source folder '{subfolder_name}' does not exist at {base_path}\")\n",
        "else:\n",
        "    # Filter for PDF files in the specified subfolder\n",
        "    pdffiles = [str(f) for f in subfolder_path.glob(\"*.pdf\") if f.is_file()]\n",
        "\n",
        "    if not pdffiles:\n",
        "        print(f\"No PDF files found in the folder '{subfolder_name}'.\")\n",
        "    else:\n",
        "        # Get the current period (YYYYMM)\n",
        "        current_date_period = input(\"Enter the Period (YYYYMM, e.g., 202507): \")\n",
        "\n",
        "        # Create the result file name\n",
        "        result_file = f\"st_{current_date_period}_STITCHED.pdf\"\n",
        "\n",
        "        # Create a PdfMerger object\n",
        "        merger = PdfMerger()\n",
        "\n",
        "        # Append the PDF files\n",
        "        print(f\"Merging {len(pdffiles)} PDF files...\")\n",
        "        for pdf in pdffiles:\n",
        "            merger.append(pdf)\n",
        "\n",
        "        # Write the merged result file\n",
        "        merged_file = output_dir / result_file\n",
        "        merger.write(str(merged_file))\n",
        "        merger.close()\n",
        "\n",
        "        print(f\"\\n✅ Merged file '{result_file}' successfully created in {OUTPUT_DRIVE_FOLDER}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# system Tesseract (Colab)\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq tesseract-ocr libtesseract-dev\n",
        "\n",
        "# python libs\n",
        "!pip install -q pymupdf pandas pytesseract pillow rapidfuzz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYaPgSNOa6yw",
        "outputId": "b5482284-1edb-45c3-a7ed-8719d7c88ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.5_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Star Hybrid Extractor v4 (layout + reporter-driven)\n",
        "# Label: st_mpe_1.0\n",
        "import fitz\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "import logging\n",
        "import pandas as pd\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from rapidfuzz import process, fuzz\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "PDF_PATH = \"/content/drive/MyDrive/Vanguard Analytics/General/Assignments/The Star/Publications/2025/09/EPAPER0109_compressed.pdf\"\n",
        "REPORTERS_CSV_DRIVE_PATH = \"/content/drive/MyDrive/Vanguard Analytics/General/Assignments/The Star/Reporters_List/Reporters_v1.0.csv\"\n",
        "OUTPUT_CSV_DRIVE_PATH = \"/content/drive/MyDrive/Vanguard Analytics/General/Assignments/The Star/Reports/EPAPER0109_STITCHED_compressed_v4.0.csv\"\n",
        "#DEBUG_SAMPLE_CSV = \"/content/drive/MyDrive/Vanguard Analytics/General/Assignments/The Star/Reports/EPAPER0109_STITCHED_debug_sample_v4.0.csv\"\n",
        "\n",
        "OCR_LANG = \"eng\"\n",
        "MIN_SELECTABLE_ALPHA = 80\n",
        "SECTION_FUZZY_THRESHOLD = 75\n",
        "REPORTER_FUZZY_THRESHOLD = 82\n",
        "\n",
        "# utility function\n",
        "# sanitize any list before joining it\n",
        "def safe_join(tokens):\n",
        "    clean = []\n",
        "    for t in tokens:\n",
        "        if isinstance(t, str):\n",
        "            clean.append(t.strip())\n",
        "        elif pd.notna(t):\n",
        "            clean.append(str(t).strip())\n",
        "    clean = [c for c in clean if c]   # remove empty strings\n",
        "    return \" \".join(clean)\n",
        "\n",
        "# your expected sections\n",
        "expected_news_sections_st = [\n",
        "    'News', 'POLITICS', 'PHOTO STORY','BUSINESS PICTORIAL', 'CORRIDORS OF POWER',\n",
        "    'NEWS GENERAL', 'NEWSBUSINESS','BIG READ','★letters','SPORTS FEATURE', '★comment','★letters','society...',\n",
        "    'TRADE', 'VOICES', 'NEWS COUNTIES','COUNTIES','CLASSIFIED',\n",
        "    'BUSINESS CONSULTANT', 'NIGHT OF COOL', 'BUSINESS', 'Sports','SPORTS LOCAL','FOOTBALL','ATHLETICS','COVER STORY',\n",
        "    'EDITORIAL', 'OPINION', 'LIFESTYLE', 'PUZZLES', 'WORLD NEWS', 'LETTERS', 'COLUMNIST','COMMENTARY','COLUMN'#,'theSTAR'\n",
        "]\n",
        "EXPECTED_SECTIONS_UP = [s.upper() for s in expected_news_sections_st]\n",
        "\n",
        "# *opic keywords (expandable later with full list)\n",
        "TOPIC_KEYWORDS = {\n",
        "    \"Health Systems & Policy\": [\n",
        "        # Core Systems & Bodies\n",
        "        \"universal health coverage\", \"uhc\", \"sha\", \"social health authority\",\n",
        "        \"shif\", \"social health insurance fund\", \"nhif\", \"health policy\",\n",
        "        \"moh\", \"ministry of health\", \"kemsa\", \"kmtc\", \"pharmacy and poisons board\",\n",
        "        \"nacada\", \"kmpdu\", \"doctors union\", \"clinical officer\", \"pharmaceutical\",\n",
        "\n",
        "        # Facilities & Care\n",
        "        \"referral hospital\", \"level 4\", \"level 5\", \"dispensary\", \"clinic\",\n",
        "        \"maternity\", \"icu\", \"hdu\", \"dialysis\", \"chemotherapy\", \"palliative\",\n",
        "        \"medical bill\", \"insurance claim\", \"access to care\",\n",
        "\n",
        "        # Public Health Issues\n",
        "        \"malaria\", \"cholera\", \"tb\", \"hiv\", \"aids\", \"pandemic\", \"outbreak\",\n",
        "        \"vaccination\", \"immunization\", \"mental health\", \"nutrition\",\n",
        "        \"maternal health\", \"infant mortality\", \"community health promoter\"\n",
        "    ],\n",
        "\n",
        "    \"Crime\": [\n",
        "        # Law Enforcement Agencies\n",
        "        \"police\", \"dci\", \"directorate of criminal investigations\", \"nps\",\n",
        "        \"eacc\", \"ethics and anti-corruption\", \"ippoa\", \"odpp\",\n",
        "        \"inspector general\", \"atpu\", \"anti-terror\", \"security operation\",\n",
        "        \"traffic police\", \"rungu\", \"gsu\",\n",
        "\n",
        "        # Judicial/Legal\n",
        "        \"arrest\", \"suspect\", \"court\", \"magistrate\", \"judge\", \"prosecution\",\n",
        "        \"bail\", \"bond\", \"ruling\", \"jail\", \"prison\", \"remand\", \"lawsuit\",\n",
        "        \"petition\", \"affidavit\", \"injunction\", \"sentence\", \"convicted\",\n",
        "        \"charge sheet\", \"case file\",\n",
        "\n",
        "        # Offenses\n",
        "        \"murder\", \"homicide\", \"femicide\", \"assault\", \"robbery\", \"theft\",\n",
        "        \"burglary\", \"fraud\", \"graft\", \"corruption\", \"bribe\", \"embezzlement\",\n",
        "        \"money laundering\", \"trafficking\", \"drug bust\", \"raid\", \"gang\",\n",
        "        \"gunshot\", \"shootout\", \"abduction\", \"kidnap\", \"losing money\", \"carjacking\"\n",
        "    ],\n",
        "\n",
        "    \"Business\": [\n",
        "        # Finance & Markets\n",
        "        \"market\", \"economy\", \"finance\", \"bond\", \"nse\", \"nairobi securities exchange\",\n",
        "        \"shareholders\", \"dividend\", \"stocks\", \"equity\", \"tbills\", \"treasury bills\",\n",
        "        \"recession\", \"economic growth\", \"trading\", \"capital markets\",\n",
        "\n",
        "        # Banking & Currency\n",
        "        \"cbk\", \"central bank\", \"shilling\", \"forex\", \"dollar\", \"exchange rate\",\n",
        "        \"inflation\", \"interest rate\", \"loan\", \"credit\", \"bank\", \"lender\", \"fintech\",\n",
        "        \"mobile money\", \"mpesa\", \"atm\", \"digital currency\", \"banking sector\",\n",
        "\n",
        "        # Corporate & Trade\n",
        "        \"revenue\", \"profit\", \"loss\", \"turnover\", \"audit\", \"tax\", \"kra\",\n",
        "        \"kenya revenue authority\", \"file returns\", \"customs\", \"imports\",\n",
        "        \"exports\", \"manufacturing\", \"epz\", \"chamber of commerce\",\n",
        "        \"supply chain\", \"consumer price index\", \"agribusiness\",\n",
        "\n",
        "        # Energy & Infrastructure\n",
        "        \"fuel price\", \"epra\", \"kplc\", \"token\", \"electricity\", \"infrastructure\",\n",
        "        \"sgr\", \"cargo\", \"logistics\", \"highway\", \"port\", \"pipeline\", \"oil prices\",\n",
        "        \"renewable energy\", \"geothermal\"\n",
        "    ],\n",
        "\n",
        "    \"Sports\": [\n",
        "        # Football\n",
        "        \"football\", \"match\", \"soccer\", \"premier league\", \"kpl\", \"fkf\",\n",
        "        \"harambee stars\", \"gor mahia\", \"afc leopards\", \"caf\", \"afcon\",\n",
        "        \"fifa\", \"cecafa\", \"super league\", \"uefa\", \"champions league\",\n",
        "        \"transfer window\", \"score\", \"penalty\", \"red card\", \"goal\",\n",
        "\n",
        "        # Athletics\n",
        "        \"athletics\", \"marathon\", \"sprint\", \"steeplechase\", \"diamond league\",\n",
        "        \"olympics\", \"world athletics\", \"kip keino\", \"relays\", \"track and field\",\n",
        "        \"world record\", \"doping\", \"runner\", \"gold medal\", \"cross country\",\n",
        "\n",
        "        # Other Sports\n",
        "        \"rugby\", \"sevens\", \"shujaa\", \"lionesses\", \"volleyball\", \"malkia strikers\",\n",
        "        \"rally\", \"wrc\", \"safari rally\", \"basketball\", \"cricket\", \"golf\",\n",
        "        \"tournament\", \"championship\", \"coach\", \"squad\", \"fixture\", \"venue\",\n",
        "        \"boxing\", \"swimming\"\n",
        "    ],\n",
        "\n",
        "    \"Education\": [\n",
        "        # Institutions & Bodies\n",
        "        \"school\", \"university\", \"college\", \"tvet\", \"ministry of education\",\n",
        "        \"tsc\", \"teachers service commission\", \"knut\", \"kuppet\", \"helb\",\n",
        "        \"kuccps\", \"kicd\", \"unions\", \"board of management\",\n",
        "\n",
        "        # Curriculum & Exams\n",
        "        \"kcse\", \"kcpe\", \"kpsea\", \"cbc\", \"competency based curriculum\",\n",
        "        \"8-4-4\", \"junior secondary\", \"jss\", \"grade 6\", \"grade 9\", \"form four\",\n",
        "        \"syllabus\", \"national exam\", \"marking scheme\", \"results slip\",\n",
        "\n",
        "        # Academic Life\n",
        "        \"graduation\", \"intake\", \"fees\", \"capitation\", \"bursary\", \"scholarship\",\n",
        "        \"chancellor\", \"vice chancellor\", \"headteacher\", \"principal\", \"lecturer\",\n",
        "        \"strike\", \"learning\", \"literacy\", \"admissions\", \"campus\", \"tuition\"\n",
        "    ],\n",
        "\n",
        "    \"Opinion\": [\n",
        "        # Formats\n",
        "        \"opinion\", \"column\", \"editorial\", \"op-ed\", \"commentary\", \"analysis\",\n",
        "        \"perspective\", \"viewpoint\", \"my view\", \"letter to the editor\",\n",
        "        \"star say\", \"nation view\", \"standard editorial\", # Common specific column names\n",
        "\n",
        "        # Tone/Context keywords\n",
        "        \"expert take\", \"guest writer\", \"contributor\", \"thought leadership\",\n",
        "        \"analysis\", \"critique\", \"in my view\", \"basing my argument\", \"ponder\",\n",
        "        \"reflect\", \"rejoinder\"\n",
        "    ]\n",
        "}\n",
        "TOPIC_KEYWORDS_LO = {k: [w.lower() for w in v] for k, v in TOPIC_KEYWORDS.items()}\n",
        "\n",
        "# ---------------- logging ----------------\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s — %(levelname)s — %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ---------------- Utilities ----------------\n",
        "def page_has_selectable_text(page, min_alpha=MIN_SELECTABLE_ALPHA):\n",
        "    txt = page.get_text(\"text\")\n",
        "    if not txt:\n",
        "        return False\n",
        "    alpha = sum(1 for c in txt if c.isalpha())\n",
        "    return alpha >= min_alpha\n",
        "\n",
        "def ocr_page(page, dpi=200):\n",
        "    mat = fitz.Matrix(dpi/72, dpi/72)\n",
        "    pix = page.get_pixmap(matrix=mat, alpha=False)\n",
        "    img = Image.open(io.BytesIO(pix.tobytes()))\n",
        "    text = pytesseract.image_to_string(img, lang=OCR_LANG)\n",
        "    return text\n",
        "\n",
        "def normalize_page_text_for_matching(text):\n",
        "    # minimal normalization for matching: collapse whitespace, remove weird spacing in uppercase headers\n",
        "    t = re.sub(r'\\s+', ' ', text)\n",
        "    t = re.sub(r'(\\b[A-Z])\\s(?=[A-Z])', r'\\1', t)  # join spaced uppercase words like B R I E F LY -> BRIEFLY\n",
        "    return t.strip()\n",
        "\n",
        "def extract_page_number_star(text, page_index):\n",
        "    lines = text.split(\"\\n\")\n",
        "    header = \" \".join(lines[:5]).strip()  # Only inspect top 5 text lines\n",
        "\n",
        "    # --- PAGE 1 RULE ---\n",
        "    if page_index == 0:\n",
        "        return 1  # The paper number is always 1 even if not printed\n",
        "\n",
        "    # --- RULE: integer at beginning (page 2,4,6…)\n",
        "    start_num = re.match(r\"^\\s*(\\d{1,2})\\b\", header)\n",
        "    if start_num:\n",
        "        return int(start_num.group(1))\n",
        "\n",
        "    # --- RULE: integer at end (page 3,5,7…)\n",
        "    end_num = re.search(r\"\\b(\\d{1,2})\\s*$\", header)\n",
        "    if end_num:\n",
        "        return int(end_num.group(1))\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_header(text, page_index=None):\n",
        "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
        "    head = \" \".join(lines[:8]) if lines else \"\"\n",
        "    # date\n",
        "    date_m = re.search(r\"(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday),?\\s+[A-Za-z]+\\s+\\d{1,2},\\s+\\d{4}\", head)\n",
        "    date = date_m.group(0) if date_m else None\n",
        "    # page number\n",
        "    # --- NEW: Use The-Star-specific page-number extractor ---\n",
        "    page_no = extract_page_number_star(text, page_index) if page_index is not None else None\n",
        "    # publication\n",
        "    pub_m = re.search(r\"THE[-\\s]*STAR|the-star\\.co\\.ke\", head, re.IGNORECASE)\n",
        "    pub = pub_m.group(0) if pub_m else None\n",
        "    # section fuzzy match\n",
        "    section = None\n",
        "    if head:\n",
        "        best = process.extractOne(head.upper(), EXPECTED_SECTIONS_UP, scorer=fuzz.partial_ratio)\n",
        "        if best and best[1] >= SECTION_FUZZY_THRESHOLD:\n",
        "            section = best[0]\n",
        "    return {\"date\": date, \"page_number\": page_no, \"publication\": pub, \"news_section\": section}\n",
        "\n",
        "# ---------------- Block / layout helpers ----------------\n",
        "# ===== BEGIN REPLACE: split_blocks_layout (stable, smaller sub-blocks) =====\n",
        "def split_blocks_layout(page):\n",
        "    \"\"\"\n",
        "    Safer block splitter that:\n",
        "      - extracts text spans per block,\n",
        "      - splits large blocks on double-newline into sub-blocks,\n",
        "      - returns list of dicts: {text, bbox, block, x0, y0, x1, y1}\n",
        "    \"\"\"\n",
        "    d = page.get_text(\"dict\")\n",
        "    blocks = []\n",
        "    for b in d.get(\"blocks\", []):\n",
        "        if b.get(\"type\") != 0:\n",
        "            continue\n",
        "        # build raw text from spans (preserve line breaks)\n",
        "        lines = []\n",
        "        for ln in b.get(\"lines\", []):\n",
        "            line_text = \"\".join(span.get(\"text\", \"\") for span in ln.get(\"spans\", []))\n",
        "            lines.append(line_text)\n",
        "        raw = \"\\n\".join(lines).strip()\n",
        "        if not raw:\n",
        "            continue\n",
        "        # split on two or more newlines to get logical sub-blocks\n",
        "        subs = re.split(r'\\n{2,}', raw)\n",
        "        for sub in subs:\n",
        "            s = sub.strip()\n",
        "            if not s:\n",
        "                continue\n",
        "            # bbox sometimes is None or not list — try-catch\n",
        "            bbox = b.get(\"bbox\") if isinstance(b.get(\"bbox\"), (list, tuple)) and len(b.get(\"bbox\")) >= 4 else [0, 0, 0, 0]\n",
        "            x0, y0, x1, y1 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
        "            blocks.append({\n",
        "                \"text\": s,\n",
        "                \"bbox\": bbox,\n",
        "                \"block\": b,\n",
        "                \"x0\": float(x0 or 0),\n",
        "                \"y0\": float(y0 or 0),\n",
        "                \"x1\": float(x1 or 0),\n",
        "                \"y1\": float(y1 or 0),\n",
        "            })\n",
        "    # Sort by page reading order (y then x)\n",
        "    blocks = sorted(blocks, key=lambda bb: (bb[\"y0\"], bb[\"x0\"]))\n",
        "    return blocks\n",
        "# ===== END REPLACE: split_blocks_layout =====\n",
        "\n",
        "# ===== BEGIN REPLACE: group_blocks_by_column (gap clustering, no sklearn) =====\n",
        "def group_blocks_by_column(blocks, column_gap_threshold=None):\n",
        "    \"\"\"\n",
        "    Backwards-compatible, robust column grouping for The Star newspaper.\n",
        "\n",
        "    - Accepts optional column_gap_threshold (keeps older callers happy).\n",
        "    - If column_gap_threshold is None, uses a sensible default (35 px).\n",
        "    - Safely extracts x0,y0 from bbox and clusters by horizontal gaps.\n",
        "    - Limits columns to MAX_COLS (5).\n",
        "    - Returns {col_index: [blocks_sorted_top_to_bottom]}\n",
        "    \"\"\"\n",
        "    if not blocks:\n",
        "        return {}\n",
        "\n",
        "    # Allow caller override; otherwise use default for The Star\n",
        "    DEFAULT_GAP = 35.0\n",
        "    COLUMN_GAP = float(column_gap_threshold) if column_gap_threshold is not None else DEFAULT_GAP\n",
        "    MAX_COLS = 5\n",
        "\n",
        "    # ---- Safe extractor for bbox coordinates ----\n",
        "    def safe_xy(b):\n",
        "        bbox = b.get(\"bbox\")\n",
        "        if bbox and isinstance(bbox, (list, tuple)) and len(bbox) >= 2:\n",
        "            try:\n",
        "                return float(bbox[0]), float(bbox[1])\n",
        "            except Exception:\n",
        "                return 0.0, 0.0\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    # Annotate blocks with x0,y0\n",
        "    annotated = []\n",
        "    for b in blocks:\n",
        "        x0, y0 = safe_xy(b)\n",
        "        annotated.append({\"x0\": x0, \"y0\": y0, \"blk\": b})\n",
        "\n",
        "    # Sort left -> right by x0\n",
        "    annotated.sort(key=lambda x: x[\"x0\"])\n",
        "\n",
        "    # If after sorting all x0 are nearly identical (OCR fallback), put everything in one column\n",
        "    x_vals = [a[\"x0\"] for a in annotated]\n",
        "    if len(x_vals) > 1 and max(x_vals) - min(x_vals) < 1.0:\n",
        "        # single column, sort top to bottom\n",
        "        col_sorted = sorted(annotated, key=lambda x: (x[\"y0\"], x[\"x0\"]))\n",
        "        return {0: [x[\"blk\"] for x in col_sorted]}\n",
        "\n",
        "    # Cluster into columns by horizontal gap\n",
        "    columns = []\n",
        "    current = [annotated[0]]\n",
        "    last_x = annotated[0][\"x0\"]\n",
        "\n",
        "    for item in annotated[1:]:\n",
        "        x0 = item[\"x0\"]\n",
        "        if (x0 - last_x) > COLUMN_GAP and len(columns) < MAX_COLS:\n",
        "            columns.append(current)\n",
        "            current = [item]\n",
        "        else:\n",
        "            current.append(item)\n",
        "        last_x = x0\n",
        "    columns.append(current)\n",
        "\n",
        "    # If we accidentally created more than MAX_COLS due to weird spacing, merge nearest columns left-to-right\n",
        "    while len(columns) > MAX_COLS:\n",
        "        # find smallest gap between consecutive column centers and merge those two\n",
        "        centers = []\n",
        "        for col in columns:\n",
        "            xs = [it[\"x0\"] for it in col]\n",
        "            centers.append(sum(xs) / len(xs) if xs else 0)\n",
        "        min_gap = None\n",
        "        merge_idx = 0\n",
        "        for i in range(len(centers)-1):\n",
        "            gap = abs(centers[i+1] - centers[i])\n",
        "            if min_gap is None or gap < min_gap:\n",
        "                min_gap = gap\n",
        "                merge_idx = i\n",
        "        # merge merge_idx and merge_idx+1\n",
        "        columns[merge_idx] = columns[merge_idx] + columns[merge_idx+1]\n",
        "        del columns[merge_idx+1]\n",
        "\n",
        "    # Now sort each column top-to-bottom and produce the map\n",
        "    col_map = {}\n",
        "    for i, col in enumerate(columns):\n",
        "        col_sorted = sorted(col, key=lambda x: (x[\"y0\"], x[\"x0\"]))\n",
        "        col_map[i] = [x[\"blk\"] for x in col_sorted]\n",
        "\n",
        "    return col_map\n",
        "\n",
        "# ===== END REPLACE: group_blocks_by_column =====\n",
        "\n",
        "def span_is_bold(span):\n",
        "    # flag bit 2 indicates bold in PyMuPDF spans (heuristic)\n",
        "    return (span.get(\"flags\", 0) & 2) != 0\n",
        "\n",
        "def extract_title_smart(blk):\n",
        "    \"\"\"\n",
        "    Improved title extractor for The Star newspaper.\n",
        "    Handles:\n",
        "      - bold spans split across multiple lines\n",
        "      - multi-line titles\n",
        "      - titles with short first lines\n",
        "      - fallback to first 2–3 lines if bold detection fails\n",
        "    \"\"\"\n",
        "    if not blk or not blk.get(\"block\"):\n",
        "        return None\n",
        "\n",
        "    block = blk[\"block\"]\n",
        "    title_lines = []\n",
        "\n",
        "    # --- STEP 1: Try bold spans anywhere in the block (not only top lines) ---\n",
        "    bold_fragments = []\n",
        "    for line in block.get(\"lines\", []):\n",
        "        for span in line.get(\"spans\", []):\n",
        "            txt = span.get(\"text\", \"\").strip()\n",
        "            if not txt:\n",
        "                continue\n",
        "            # Bold or slightly larger font indicates title\n",
        "            if span_is_bold(span) or span.get(\"size\", 0) >= 11.0:\n",
        "                bold_fragments.append(txt)\n",
        "\n",
        "    if bold_fragments:\n",
        "        title = \" \".join(bold_fragments).strip()\n",
        "        # Require only 2+ words for a valid title\n",
        "        if len(title.split()) >= 2 and len(title) <= 150:\n",
        "            return title\n",
        "\n",
        "    # --- STEP 2: If bold failed, use first 3 lines as candidate title area ---\n",
        "    raw_lines = [l.strip() for l in blk[\"text\"].splitlines() if l.strip()]\n",
        "\n",
        "    if not raw_lines:\n",
        "        return None\n",
        "\n",
        "    # take first 3 non-empty lines\n",
        "    candidate = \" \".join(raw_lines[:3]).strip()\n",
        "\n",
        "    # remove byline-like patterns\n",
        "    candidate = re.sub(r\"^\\s*BY\\s+.+\", \"\", candidate, flags=re.IGNORECASE)\n",
        "\n",
        "    # ensure at least 2 words and reasonable length\n",
        "    if len(candidate.split()) >= 2 and len(candidate) <= 150:\n",
        "        return candidate\n",
        "\n",
        "    # --- STEP 3: Last resort: first non-empty line with ≥2 words ---\n",
        "    for ln in raw_lines:\n",
        "        if len(ln.split()) >= 2 and len(ln) <= 150:\n",
        "            return ln\n",
        "\n",
        "    return None\n",
        "\n",
        "def is_likely_byline(line):\n",
        "    ln = line.strip()\n",
        "    if not ln:\n",
        "        return False\n",
        "    if ln.upper().startswith(\"BY \") and len(ln.split()) <= 12:\n",
        "        return True\n",
        "    if ln.isupper() and 2 <= len(ln.split()) <= 6 and len(ln) < 80:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# --- REVERTED/ORIGINAL detect_photo_credit_anywhere for plain text ---\n",
        "import re\n",
        "\n",
        "def detect_photo_credit_anywhere(block):\n",
        "  \"\"\"\n",
        "  Robust universal photo credit detector for The Star:\n",
        "  - detects uppercase or proper-case names\n",
        "  - detects '/ENOS TECHE', '/Faith Matete', '/PCS', '/FILE', '/HANDOUT'\n",
        "  - supports slash appearing anywhere in the block\n",
        "  - supports broken spans, irregular spacing\n",
        "  \"\"\"\n",
        "\n",
        "  # --- Extract text from spans if block dict provided ---\n",
        "  if isinstance(block, dict) and \"block\" in block:\n",
        "    spans = []\n",
        "    for line in block[\"block\"].get(\"lines\", []):\n",
        "      for span in line.get(\"spans\", []):\n",
        "        txt = span.get(\"text\", \"\")\n",
        "        spans.append(txt)\n",
        "    text = \" \".join(spans)\n",
        "  else:\n",
        "    text = str(block) if isinstance(block, str) else \"\"\n",
        "\n",
        "  # Normalize whitespace\n",
        "  text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "  # Combined robust pattern:\n",
        "  # /NAME\n",
        "  # / FILE\n",
        "  # / HANDOUT\n",
        "  # / Faith Matete\n",
        "  # /Enos TecHe (case-insensitive)\n",
        "  photo_regex = re.compile(\n",
        "    r\"/\\s*(?:\"\n",
        "    r\"[A-Z][A-Za-z]+(?:\\s+[A-Z][A-Za-z]+)*\"  # Proper case name (Faith Matete)\n",
        "    r\"|[A-Z]{2,}\"                # All caps (PCS, FILE)\n",
        "    r\"|FILE|HANDOUT|PCS|ENOS\\s+TECHE\"      # explicit common variants\n",
        "    r\")\",\n",
        "    flags=re.IGNORECASE\n",
        "  )\n",
        "\n",
        "  m = photo_regex.search(text)\n",
        "  if m:\n",
        "    # Normalize spacing inside the credit\n",
        "    return re.sub(r\"\\s+\", \" \", m.group(0)).strip()\n",
        "\n",
        "  return None\n",
        "\n",
        "# --- NEW HELPER FOR BOLD PHOTO CREDIT FROM BLOCKS ---\n",
        "def detect_bold_photo_credit_from_block(blk_obj):\n",
        "    \"\"\"\n",
        "    Detects a photo credit in bold spans within a PyMuPDF block object.\n",
        "    Now supports delimiters '/' and 'COMPILED BY '.\n",
        "    \"\"\"\n",
        "    if not blk_obj or not blk_obj.get(\"block\"):\n",
        "        return None\n",
        "\n",
        "    # Regex to detect either \"/\" or \"COMPILED BY \"\n",
        "    # followed by uppercase words, proper-case names, or known variants\n",
        "    credit_regex = re.compile(\n",
        "        r\"(?:/|COMPILED BY )\\s*(?:\"\n",
        "        r\"[A-Z][A-Za-z]+(?:\\s+[A-Z][A-Za-z]+)*\"     # Proper case names\n",
        "        r\"|[A-Z]{2,}\"                                # All caps words\n",
        "        r\"|FILE|HANDOUT|PCS|ENOS\\s+TECHE\"            # Explicit known variants\n",
        "        r\")\",\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    for line in blk_obj[\"block\"].get(\"lines\", []):\n",
        "        for span in line.get(\"spans\", []):\n",
        "            if span_is_bold(span):\n",
        "                text_in_span = span.get(\"text\", \"\").strip()\n",
        "                m = credit_regex.search(text_in_span)\n",
        "                if m:\n",
        "                    return re.sub(r\"\\s+\", \" \", m.group(0)).strip()\n",
        "\n",
        "    return None\n",
        "\n",
        "# ---------------- Reporter loading & matching (from DN logic) ----------------\n",
        "def load_reporters(path):\n",
        "    # Load CSV or Excel\n",
        "    if path.lower().endswith(tuple([\".xls\", \".xlsx\"])):\n",
        "        df = pd.read_excel(path)\n",
        "    else:\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "    # Validate required columns\n",
        "    expected_cols = [\"Reporter\", \"Reporter_Category\", \"Publication_Section\"]\n",
        "    missing = [c for c in expected_cols if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Reporters CSV missing expected columns: {missing}\")\n",
        "\n",
        "    # ---- CLEANING BLOCK (Fix for numpy.float64 / NaN issues) ----\n",
        "    df[\"Reporter\"] = df[\"Reporter\"].astype(str)           # convert all to string\n",
        "    df[\"Reporter\"] = df[\"Reporter\"].str.strip()           # trim spaces\n",
        "    df = df[df[\"Reporter\"].str.lower() != \"nan\"]          # drop 'nan' strings\n",
        "    df = df[df[\"Reporter\"] != \"\"]                         # drop empty strings\n",
        "    df = df.dropna(subset=[\"Reporter\"])                   # drop actual NaN\n",
        "    # --------------------------------------------------------------\n",
        "\n",
        "    # Normalize reporter names\n",
        "    df[\"Reporter_norm\"] = (\n",
        "        df[\"Reporter\"]\n",
        "        .astype(str)\n",
        "        .str.strip()\n",
        "        .replace(r\"\\s+\", \" \", regex=True)    # unwarp multi spaces\n",
        "    )\n",
        "\n",
        "    df[\"Reporter_norm_lower\"] = df[\"Reporter_norm\"].str.lower()\n",
        "\n",
        "    # Output clean lists\n",
        "    reporters_list = df[\"Reporter_norm\"].tolist()         # proper names\n",
        "    reporters_list_lower = df[\"Reporter_norm_lower\"].tolist()\n",
        "\n",
        "    return df, reporters_list, reporters_list_lower\n",
        "\n",
        "REPORTERS_DF, REPORTERS_LIST, REPORTERS_LIST_LOWER = load_reporters(REPORTERS_CSV_DRIVE_PATH)\n",
        "logger.info(f\"Loaded {len(REPORTERS_LIST)} reporters from CSV.\")\n",
        "\n",
        "def create_name_pattern_flexible(reporter_name):\n",
        "    parts = reporter_name.split()\n",
        "    if not parts:\n",
        "        return re.compile(r'(?!x)x')\n",
        "    pattern_str = r'\\s*'.join([re.escape(word) for word in parts])\n",
        "    return re.compile(pattern_str, re.IGNORECASE)\n",
        "\n",
        "def normalize_byline_text(raw_byline):\n",
        "    if not raw_byline:\n",
        "        return []\n",
        "    s = raw_byline.strip()\n",
        "    s = re.sub(r'^\\s*BY[.\\s:,-]*', '', s, flags=re.IGNORECASE)\n",
        "    s = s.replace(\"/\", \" \").replace(\"—\", \" \").replace(\"–\", \" \").strip()\n",
        "    s = re.sub(r'\\s{2,}', ' ', s)\n",
        "    parts = re.split(r'\\s+(?:AND|&|,|;)\\s+|\\n', s, flags=re.IGNORECASE)\n",
        "    parts = [p.strip() for p in parts if p and len(p.strip())>1]\n",
        "    seen = set(); out = []\n",
        "    for p in parts:\n",
        "        key = re.sub(r'\\s+', ' ', p).strip()\n",
        "        if key.lower() not in seen:\n",
        "            out.append(key); seen.add(key.lower())\n",
        "    return out\n",
        "\n",
        "def match_single_name(name_candidate):\n",
        "    if not name_candidate:\n",
        "        return None, None, None, 0\n",
        "    cand = re.sub(r'\\s+', ' ', name_candidate.strip())\n",
        "    cand_lower = cand.lower()\n",
        "    try:\n",
        "        idx = REPORTERS_LIST_LOWER.index(cand_lower)\n",
        "    except ValueError:\n",
        "        idx = None\n",
        "    if idx is not None:\n",
        "        row = REPORTERS_DF.iloc[idx]\n",
        "        return row[\"Reporter_norm\"], row.get(\"Reporter_Category\", None), row.get(\"Publication_Section\", None), 100\n",
        "    best = process.extractOne(cand_lower, REPORTERS_LIST_LOWER, scorer=fuzz.WRatio)\n",
        "    if best and best[1] >= REPORTER_FUZZY_THRESHOLD:\n",
        "        matched_lower = best[0]\n",
        "        idx = REPORTERS_LIST_LOWER.index(matched_lower)\n",
        "        row = REPORTERS_DF.iloc[idx]\n",
        "        return row[\"Reporter_norm\"], row.get(\"Reporter_Category\", None), row.get(\"Publication_Section\", None), best[1]\n",
        "    return None, None, None, 0\n",
        "\n",
        "def match_reporter(raw_byline):\n",
        "    candidates = normalize_byline_text(raw_byline)\n",
        "    matched_names=[]; cats=[]; pubs=[]; scores=[]\n",
        "    for cand in candidates:\n",
        "        mname, mcat, mpub, score = match_single_name(cand)\n",
        "        matched_names.append(mname); cats.append(mcat); pubs.append(mpub); scores.append(score)\n",
        "    return {\"matched_names\": matched_names, \"reporter_categories\": cats, \"publication_sections\": pubs, \"match_scores\": scores}\n",
        "\n",
        "# ---------------- Topic matching ----------------\n",
        "def match_topic(text, section_hint=None):\n",
        "    if not text: return \"other\"\n",
        "    low = text.lower()\n",
        "    for topic, keywords in TOPIC_KEYWORDS_LO.items():\n",
        "        for kw in sorted(keywords, key=lambda x: -len(x)):\n",
        "            if kw in low:\n",
        "                return topic\n",
        "    # fallback by section hint\n",
        "    if section_hint:\n",
        "        sh = (section_hint or \"\").lower()\n",
        "        if \"sport\" in sh: return \"Sports\"\n",
        "        if \"business\" in sh: return \"Business\"\n",
        "    return \"other\"\n",
        "\n",
        "# ---------------- Hybrid page -> article logic ----------------\n",
        "import re\n",
        "# Assuming pandas (pd) is imported elsewhere for REPORTERS_DF, and logger is available.\n",
        "# The full function relies on several external functions (e.g., split_blocks_layout, match_reporter, etc.)\n",
        "# which are assumed to be defined elsewhere in your script.\n",
        "\n",
        "# --- Helper Function to Get All Known Credit Terms ---\n",
        "# NOTE: This relies on REPORTERS_LIST being available (e.g., globally or passed in).\n",
        "import re\n",
        "\n",
        "# Assume KNOWN_CREDITS helper is defined as before\n",
        "def get_known_credits_list():\n",
        "    \"\"\"Returns a list of all names/sources that should be in the photo_credit column.\"\"\"\n",
        "    known_list = list(REPORTERS_LIST) if 'REPORTERS_LIST' in globals() else []\n",
        "    known_list.extend([\"HANDOUT\", \"FILE\", \"AP\", \"REUTERS\", \"MPE\", \"STAR\", \"COURTESY\"])\n",
        "    return known_list\n",
        "\n",
        "# --- The Core Photo Credit Cleanup Function (Updated) ---\n",
        "def clean_photo_credit(credit_text):\n",
        "    \"\"\"\n",
        "    Cleans up the raw photo credit string aggressively by retaining only the\n",
        "    first matched known name/source and immediately cutting off all following text.\n",
        "    \"\"\"\n",
        "    if not credit_text:\n",
        "        return None\n",
        "\n",
        "    # Normalize the text (uppercase, strip leading/trailing slashes and spaces)\n",
        "    normalized_credit = credit_text.strip().upper().strip('/')\n",
        "\n",
        "    KNOWN_CREDITS = get_known_credits_list()\n",
        "\n",
        "    # 1. Create a dynamic regex pattern for all known core credit terms\n",
        "    # The pattern looks for the term followed by an optional space/separator\n",
        "    # and then captures EVERYTHING ELSE (.*) to be replaced.\n",
        "\n",
        "    # Create the search group: (FILE|HANDOUT|TEDDY MULEI|...)\n",
        "    credit_group = '|'.join(re.escape(c.upper()) for c in KNOWN_CREDITS if c)\n",
        "\n",
        "    # Create the pattern: Look for a word boundary, the credit term, an optional separator/space,\n",
        "    # and then capture everything that follows.\n",
        "    # The (?:...) non-capturing group is for the optional slash or space after the credit.\n",
        "    # Example: (\\bFILE\\s?)(.*)\n",
        "\n",
        "    # Look for the core credit at the start or after a space/slash.\n",
        "    match = re.search(r'\\b(' + credit_group + r')\\b(?:/|\\s|$)', normalized_credit)\n",
        "\n",
        "    if match:\n",
        "        matched_term = match.group(1).strip()\n",
        "\n",
        "        # Aggressive Cutoff: Find the position immediately after the matched term\n",
        "        # and forcefully return only the text up to that point.\n",
        "\n",
        "        # Calculate the position right after the matched credit term\n",
        "        start_pos = match.start(1)\n",
        "        end_pos = match.end(1)\n",
        "\n",
        "        # We need to ensure we capture the full word, and nothing more.\n",
        "        clean_text = normalized_credit[start_pos:end_pos].strip()\n",
        "\n",
        "        # Perform final cleanup to remove trailing words like 'BY' or a slash, but only if they\n",
        "        # immediately follow the term in the original raw text.\n",
        "\n",
        "        # Remove common prefixes from the clean result if they were accidentally included\n",
        "        for prefix in ['BY', 'FOR', 'FROM']:\n",
        "             if clean_text.startswith(prefix + ' '):\n",
        "                 clean_text = clean_text[len(prefix) + 1:].strip()\n",
        "\n",
        "        return clean_text\n",
        "\n",
        "    # 2. Fallback: If no known credit term was found\n",
        "    for prefix in ['BY', 'FOR', 'FROM', 'PHOTO:']:\n",
        "        if normalized_credit.startswith(prefix):\n",
        "            normalized_credit = normalized_credit[len(prefix):].strip()\n",
        "\n",
        "    # Return the first 3 words of the fallback\n",
        "    return ' '.join(normalized_credit.split()[:3]).strip().strip('/')\n",
        "\n",
        "\n",
        "# --- The Main Article Extraction Function ---\n",
        "import re\n",
        "# Assuming pandas (pd) is imported elsewhere for REPORTERS_DF, and logger is available.\n",
        "# The full function relies on several external functions (e.g., split_blocks_layout, match_reporter, etc.)\n",
        "# which are assumed to be defined elsewhere in your script.\n",
        "\n",
        "# --- Helper Function to Get All Known Credit Terms ---\n",
        "# NOTE: This relies on REPORTERS_LIST being available (e.g., globally or passed in).\n",
        "import re\n",
        "\n",
        "# Assume KNOWN_CREDITS helper is defined as before\n",
        "def get_known_credits_list():\n",
        "    \"\"\"Returns a list of all names/sources that should be in the photo_credit column.\"\"\"\n",
        "    known_list = list(REPORTERS_LIST) if 'REPORTERS_LIST' in globals() else []\n",
        "    known_list.extend([\"HANDOUT\", \"FILE\", \"AP\", \"REUTERS\", \"MPE\", \"STAR\", \"COURTESY\"])\n",
        "    return known_list\n",
        "\n",
        "# --- The Core Photo Credit Cleanup Function (Updated) ---\n",
        "def clean_photo_credit(credit_text):\n",
        "    \"\"\"\n",
        "    Cleans up the raw photo credit string aggressively by retaining only the\n",
        "    first matched known name/source and immediately cutting off all following text.\n",
        "    \"\"\"\n",
        "    if not credit_text:\n",
        "        return None\n",
        "\n",
        "    # Normalize the text (uppercase, strip leading/trailing slashes and spaces)\n",
        "    normalized_credit = credit_text.strip().upper().strip('/')\n",
        "\n",
        "    KNOWN_CREDITS = get_known_credits_list()\n",
        "\n",
        "    # 1. Create a dynamic regex pattern for all known core credit terms\n",
        "    # The pattern looks for the term followed by an optional space/separator\n",
        "    # and then captures EVERYTHING ELSE (.*) to be replaced.\n",
        "\n",
        "    # Create the search group: (FILE|HANDOUT|TEDDY MULEI|...)\n",
        "    credit_group = '|'.join(re.escape(c.upper()) for c in KNOWN_CREDITS if c)\n",
        "\n",
        "    # Create the pattern: Look for a word boundary, the credit term, an optional separator/space,\n",
        "    # and then capture everything that follows.\n",
        "    # The (?:...) non-capturing group is for the optional slash or space after the credit.\n",
        "    # Example: (\\bFILE\\s?)(.*)\n",
        "\n",
        "    # Look for the core credit at the start or after a space/slash.\n",
        "    match = re.search(r'\\b(' + credit_group + r')\\b(?:/|\\s|$)', normalized_credit)\n",
        "\n",
        "    if match:\n",
        "        matched_term = match.group(1).strip()\n",
        "\n",
        "        # Aggressive Cutoff: Find the position immediately after the matched term\n",
        "        # and forcefully return only the text up to that point.\n",
        "\n",
        "        # Calculate the position right after the matched credit term\n",
        "        start_pos = match.start(1)\n",
        "        end_pos = match.end(1)\n",
        "\n",
        "        # We need to ensure we capture the full word, and nothing more.\n",
        "        clean_text = normalized_credit[start_pos:end_pos].strip()\n",
        "\n",
        "        # Perform final cleanup to remove trailing words like 'BY' or a slash, but only if they\n",
        "        # immediately follow the term in the original raw text.\n",
        "\n",
        "        # Remove common prefixes from the clean result if they were accidentally included\n",
        "        for prefix in ['BY', 'FOR', 'FROM']:\n",
        "             if clean_text.startswith(prefix + ' '):\n",
        "                 clean_text = clean_text[len(prefix) + 1:].strip()\n",
        "\n",
        "        return clean_text\n",
        "\n",
        "    # 2. Fallback: If no known credit term was found\n",
        "    for prefix in ['BY', 'FOR', 'FROM', 'PHOTO:']:\n",
        "        if normalized_credit.startswith(prefix):\n",
        "            normalized_credit = normalized_credit[len(prefix):].strip()\n",
        "\n",
        "    # Return the first 3 words of the fallback\n",
        "    return ' '.join(normalized_credit.split()[:3]).strip().strip('/')\n",
        "\n",
        "\n",
        "# --- The Main Article Extraction Function ---\n",
        "# ===== BEGIN REPLACE: page_to_articles_hybrid (v5 — deterministic assembly) =====\n",
        "def page_to_articles_hybrid(page, page_index):\n",
        "    \"\"\"\n",
        "    Layout-first + reporter-driven extraction for The Star articles.\n",
        "    Refined for robust block splitting, title filtering, and deduplication.\n",
        "    \"\"\"\n",
        "\n",
        "    use_ocr = not page_has_selectable_text(page)\n",
        "    if use_ocr:\n",
        "        logger.info(f\"Page {page_index} needs OCR\")\n",
        "        page_text = ocr_page(page)\n",
        "        header = extract_header(page_text, page_index)\n",
        "        blocks_raw = [{\"text\": p.strip(), \"bbox\": None, \"block\": None} for p in page_text.split(\"\\n\\n\") if len(p.strip())>40]\n",
        "    else:\n",
        "        page_text = page.get_text(\"text\")\n",
        "        header = extract_header(page_text)\n",
        "        blocks_raw = split_blocks_layout(page)\n",
        "\n",
        "    # --- 1. Refine Blocks: Split on Internal Bylines (Robust Split) ---\n",
        "    blocks = []\n",
        "    # Regex for a byline: Optional newline/start, whitespace, \"BY\" + (2 to 4 uppercase words), optional whitespace/newline\n",
        "    byline_split_pattern = re.compile(r'(?:\\n|^)\\s*(BY\\s+(?:[A-Z]+\\s*){2,4})\\s*(?:\\n|$)', re.IGNORECASE)\n",
        "\n",
        "    for b in blocks_raw:\n",
        "        raw_txt = b.get(\"text\", \"\")\n",
        "        if byline_split_pattern.search(raw_txt):\n",
        "            parts = byline_split_pattern.split(raw_txt)\n",
        "\n",
        "            # First part (text before the first byline)\n",
        "            if parts[0].strip():\n",
        "                blocks.append({\"text\": parts[0].strip(), \"bbox\": b.get(\"bbox\"), \"block\": b.get(\"block\")})\n",
        "\n",
        "            # Iterate over the rest in pairs (Byline, Text)\n",
        "            for i in range(1, len(parts), 2):\n",
        "                byline_txt = parts[i]\n",
        "                body_txt = parts[i+1] if i+1 < len(parts) else \"\"\n",
        "                full_chunk = f\"{byline_txt}\\n{body_txt}\"\n",
        "                if full_chunk.strip():\n",
        "                    blocks.append({\"text\": full_chunk.strip(), \"bbox\": b.get(\"bbox\"), \"block\": b.get(\"block\")})\n",
        "        else:\n",
        "            blocks.append(b)\n",
        "\n",
        "    articles = []\n",
        "    page_text_norm = normalize_page_text_for_matching(page_text).lower()\n",
        "    compiled_names = [(r, create_name_pattern_flexible(r)) for r in REPORTERS_LIST]\n",
        "\n",
        "    # --- Layout-first pass ---\n",
        "    for blk in blocks:\n",
        "        raw = blk.get(\"text\", \"\")\n",
        "        if len(raw.split()) < 12:\n",
        "            continue\n",
        "\n",
        "        # --- 2. Title Fix: Prevent \"BY [NAME]\" from being the title ---\n",
        "        title = extract_title_smart(blk)\n",
        "        # CRITICAL FILTER: If the title looks like a byline, reject it.\n",
        "        if title and re.match(r'^\\s*BY\\s+', title, re.IGNORECASE):\n",
        "            title = None\n",
        "\n",
        "        # Backward search for title (also filtering for bylines)\n",
        "        if not title:\n",
        "            try:\n",
        "                idx_blk = blocks.index(blk)\n",
        "            except ValueError:\n",
        "                idx_blk = None\n",
        "            if idx_blk is not None:\n",
        "                # Look backwards\n",
        "                for back in range(max(0, idx_blk-6), idx_blk):\n",
        "                    # Check candidate block for title, strictly ignoring bylines\n",
        "                    t2 = extract_title_smart(blocks[back])\n",
        "                    if t2 and not re.match(r'^\\s*BY\\s+', t2, re.IGNORECASE):\n",
        "                        title = t2\n",
        "                        break\n",
        "                # Final fallback for top headlines (also filtering)\n",
        "                if not title:\n",
        "                    for back in range(0, min(6, len(blocks))):\n",
        "                        t2 = extract_title_smart(blocks[back])\n",
        "                        if t2 and len(t2.split()) <= 20 and not re.match(r'^\\s*BY\\s+', t2, re.IGNORECASE):\n",
        "                            title = t2\n",
        "                            break\n",
        "\n",
        "        # --- Detect candidate bylines ---\n",
        "        lines = [l.strip() for l in raw.splitlines() if l.strip()]\n",
        "        candidate_bylines = []\n",
        "        for ln in lines[:14]:\n",
        "            L = ln.strip()\n",
        "            if re.match(r'^\\s*BY\\b', L, re.IGNORECASE):\n",
        "                candidate_bylines.append(L)\n",
        "                continue\n",
        "            if re.match(r'^([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,3})$', L):\n",
        "                candidate_bylines.append(L)\n",
        "                continue\n",
        "            if L.isupper() and 2 <= len(L.split()) <= 6:\n",
        "                candidate_bylines.append(L)\n",
        "                continue\n",
        "\n",
        "        # --- Match candidate bylines ---\n",
        "        found_reporters = []\n",
        "        for cand in candidate_bylines:\n",
        "            mr = match_reporter(cand)\n",
        "            if any(s > 0 for s in mr[\"match_scores\"]):\n",
        "                found_reporters.append((cand, mr))\n",
        "\n",
        "        # --- Fallback match in block ---\n",
        "        if not found_reporters:\n",
        "            for r, pat in compiled_names:\n",
        "                if pat.search(raw):\n",
        "                    idx = REPORTERS_LIST.index(r)\n",
        "                    row = REPORTERS_DF.iloc[idx]\n",
        "                    mr = {\n",
        "                        \"matched_names\": [row[\"Reporter_norm\"]],\n",
        "                        \"reporter_categories\": [row.get(\"Reporter_Category\", None)],\n",
        "                        \"publication_sections\": [row.get(\"Publication_Section\", None)],\n",
        "                        \"match_scores\": [100]\n",
        "                    }\n",
        "                    found_reporters.append((row[\"Reporter_norm\"], mr))\n",
        "\n",
        "        # --- Build article(s) ---\n",
        "        if found_reporters:\n",
        "            # ... Reporter aggregation logic ...\n",
        "\n",
        "            matched_names = []\n",
        "            cats = []\n",
        "            pubs = []\n",
        "            scores = []\n",
        "            byline_raw_combined = []\n",
        "            for cand, mr in found_reporters:\n",
        "                byline_raw_combined.append(cand)\n",
        "                matched_names.extend([n for n in mr[\"matched_names\"] if n])\n",
        "                cats = [str(c).strip() for c in cats if pd.notna(c) and str(c).strip()]\n",
        "                pubs = [str(p).strip() for p in pubs if pd.notna(p) and str(p).strip()]\n",
        "                scores.extend([s for s in mr[\"match_scores\"]])\n",
        "\n",
        "            photo_credit = detect_bold_photo_credit_from_block(blk) or detect_photo_credit_anywhere(raw)\n",
        "\n",
        "            # Determine Types\n",
        "            # Determine Types\n",
        "            wc = len(raw.split())\n",
        "\n",
        "            # Specific handler for \"Handout\" reporter false positives\n",
        "            if len(matched_names) == 1 and matched_names[0].upper() in [\"HANDOUT\", \"FILE\", \"AP\", \"REUTERS\"]:\n",
        "                 art_type = \"PHOTO\"\n",
        "            elif wc > 250:\n",
        "                # If word count is high, it's definitely an article\n",
        "                art_type = \"MULTI-AUTHOR\" if len(matched_names) > 1 else \"SINGLE-AUTHOR\"\n",
        "            elif wc > 100:\n",
        "                # If word count is moderate, check for photo credit\n",
        "                if photo_credit:\n",
        "                    art_type = \"PHOTO\" # Treat as a long caption/short photo story\n",
        "                else:\n",
        "                    art_type = \"MULTI-AUTHOR\" if len(matched_names) > 1 else \"SINGLE-AUTHOR\"\n",
        "            elif photo_credit or wc < 100:\n",
        "                # If short, or has a photo credit, classify as PHOTO unless it's the only text block\n",
        "                art_type = \"PHOTO\"\n",
        "            elif matched_names:\n",
        "                art_type = \"MULTI-AUTHOR\" if len(matched_names) > 1 else \"SINGLE-AUTHOR\"\n",
        "            else:\n",
        "                art_type = \"PHOTO\" if photo_credit else \"SINGLE-AUTHOR\"\n",
        "\n",
        "            # --- Strict Body Extension (Stop if new article starts) ---\n",
        "            body = raw\n",
        "            try:\n",
        "                idx_blk = blocks.index(blk)\n",
        "            except ValueError:\n",
        "                idx_blk = None\n",
        "            if idx_blk is not None:\n",
        "                for j in range(idx_blk+1, min(idx_blk+4, len(blocks))):\n",
        "                    follow = blocks[j]\n",
        "                    follow_raw = follow.get(\"text\", \"\")\n",
        "\n",
        "                    # STOP CHECK 1: If next block contains a Reporter Pattern or starts with \"BY \"\n",
        "                    if any(pat.search(follow_raw) for _, pat in compiled_names) or re.match(r'^\\s*BY\\s+', follow_raw, re.IGNORECASE):\n",
        "                        break\n",
        "\n",
        "                    # STOP CHECK 2: If next block has a title (not a byline)\n",
        "                    t_follow = extract_title_smart(follow)\n",
        "                    if t_follow and not re.match(r'^\\s*BY\\s+', t_follow, re.IGNORECASE):\n",
        "                        break\n",
        "\n",
        "                    if len(follow_raw.split()) < 20:\n",
        "                        continue\n",
        "\n",
        "                    body += \"\\n\\n\" + follow_raw\n",
        "\n",
        "            art = {\n",
        "                \"header\": header,\n",
        "                \"article_title\": title,\n",
        "                \"byline_raw\": \"; \".join(byline_raw_combined),\n",
        "                \"matched_reporter\": \"; \".join(matched_names) if matched_names else None,\n",
        "                \"reporter_category\": \"; \".join(cats) if cats else None,\n",
        "                \"publication_section_from_reporter\": \"; \".join(pubs) if pubs else None,\n",
        "                \"reporter_match_scores\": \";\".join([str(s) for s in scores]) if scores else None,\n",
        "                \"article_type\": art_type,\n",
        "                \"photo_credit\": photo_credit,\n",
        "                \"body\": body\n",
        "            }\n",
        "            articles.append(art)\n",
        "\n",
        "    # --- Fallback: scan page text for REPEAT reporters (Find all instances) ---\n",
        "    extracted_bodies = [a.get(\"body\", \"\") for a in articles]\n",
        "\n",
        "    for r in REPORTERS_LIST:\n",
        "        pat = create_name_pattern_flexible(r)\n",
        "\n",
        "        for m in pat.finditer(page_text_norm):\n",
        "            start = m.start()\n",
        "\n",
        "            # Context check: Is this hit already inside an extracted article? (Snippet must be > 50 chars)\n",
        "            snippet_check = page_text_norm[start:start+50]\n",
        "            if any(snippet_check in normalize_page_text_for_matching(b).lower() for b in extracted_bodies):\n",
        "                continue\n",
        "\n",
        "            remaining = page_text_norm[start:]\n",
        "\n",
        "            # Heuristic stops (email, horizontal rule, next byline)\n",
        "            limit = len(remaining)\n",
        "            email_m = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', remaining)\n",
        "            horiz_m = re.search(r'-{2,}', remaining)\n",
        "            next_byline = re.search(r'\\n\\s*BY\\s+', remaining[20:], re.IGNORECASE)\n",
        "\n",
        "            if email_m: limit = min(limit, email_m.start())\n",
        "            if horiz_m: limit = min(limit, horiz_m.start())\n",
        "            if next_byline: limit = min(limit, next_byline.start() + 20)\n",
        "\n",
        "            body = remaining[:limit].strip()\n",
        "\n",
        "            if len(body.split()) < 20:\n",
        "                continue\n",
        "\n",
        "            # ... Fallback article creation logic ...\n",
        "\n",
        "            try:\n",
        "                idx = REPORTERS_LIST.index(r)\n",
        "                row = REPORTERS_DF.iloc[idx]\n",
        "                matched_name = row[\"Reporter_norm\"]\n",
        "                cat = row.get(\"Reporter_Category\", None)\n",
        "                pubsec = row.get(\"Publication_Section\", None)\n",
        "            except Exception:\n",
        "                matched_name, cat, pubsec = r, None, None\n",
        "\n",
        "            photo_credit_fallback = detect_photo_credit_anywhere(body)\n",
        "            wc_fallback = len(body.split())\n",
        "\n",
        "            if wc_fallback > 200:\n",
        "                art_type = \"SINGLE-AUTHOR\"\n",
        "            elif photo_credit_fallback and wc_fallback < 80:\n",
        "                art_type = \"PHOTO\"\n",
        "            else:\n",
        "                art_type = \"SINGLE-AUTHOR\"\n",
        "\n",
        "            art = {\n",
        "                \"header\": header,\n",
        "                \"article_title\": None,\n",
        "                \"byline_raw\": matched_name,\n",
        "                \"matched_reporter\": matched_name,\n",
        "                \"reporter_category\": cat,\n",
        "                \"publication_section_from_reporter\": pubsec,\n",
        "                \"reporter_match_scores\": 100,\n",
        "                \"article_type\": art_type,\n",
        "                \"photo_credit\": photo_credit_fallback,\n",
        "                \"body\": body\n",
        "            }\n",
        "            articles.append(art)\n",
        "            extracted_bodies.append(body)\n",
        "\n",
        "    # --- Dedupe articles (FINAL FIX: Robust 200-char snippet) ---\n",
        "    unique = []\n",
        "    seen = set()\n",
        "    for a in articles:\n",
        "        # Use a long snippet (200 chars) to distinguish between overlapping or closely spaced duplicate blocks\n",
        "        body_snippet = (a.get(\"body\",\"\")[:200] or \"\").strip().lower()\n",
        "        key = ((a.get(\"matched_reporter\") or \"\").lower(), body_snippet)\n",
        "\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        unique.append(a)\n",
        "\n",
        "    # --- Enrich articles and Clean Photo Credit ---\n",
        "    rows = []\n",
        "    for a in unique:\n",
        "        hdr = a.get(\"header\", {})\n",
        "        title = a.get(\"article_title\") or (a.get(\"body\",\" \").splitlines()[0] if a.get(\"body\") else None)\n",
        "        topic = match_topic(a.get(\"body\",\" \"), section_hint=hdr.get(\"news_section\"))\n",
        "        word_count = len(a.get(\"body\",\" \").split())\n",
        "\n",
        "        raw_photo_credit = a.get(\"photo_credit\")\n",
        "        cleaned_photo_credit = clean_photo_credit(raw_photo_credit)\n",
        "\n",
        "        row = {\n",
        "            \"date\": hdr.get(\"date\"),\n",
        "            \"page_number\": page_index,\n",
        "            \"news_section\": hdr.get(\"news_section\"),\n",
        "            \"article_title\": title,\n",
        "            \"byline_raw\": a.get(\"byline_raw\"),\n",
        "            \"reporter_matched\": a.get(\"matched_reporter\"),\n",
        "            \"reporter_match_scores\": a.get(\"reporter_match_scores\"),\n",
        "            \"article_type\": a.get(\"article_type\"),\n",
        "            \"photo_credit\": cleaned_photo_credit,\n",
        "            \"topic\": topic,\n",
        "            \"word_count\": word_count,\n",
        "            \"excerpt\": a.get(\"body\",\" \")[:400]\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    return rows\n",
        "# ===== END REPLACE: page_to_articles_hybrid =====\n",
        "\n",
        "# ---------------- Main extraction ----------------\n",
        "def extract_newspaper_metadata(pdf_path):\n",
        "    logger.info(f\"Opening PDF: {pdf_path}\")\n",
        "    doc = fitz.open(pdf_path)\n",
        "    all_rows = []\n",
        "    for i in range(len(doc)):\n",
        "        page = doc.load_page(i)\n",
        "        logger.info(f\"Processing page {i+1}/{len(doc)}\")\n",
        "        # decide OCR or not & get header\n",
        "        raw_text = page.get_text(\"text\") if page_has_selectable_text(page) else ocr_page(page)\n",
        "        header = extract_header(raw_text, i+1)\n",
        "        rows = page_to_articles_hybrid(page, i+1)\n",
        "        # set header fallback if missing per row\n",
        "        for r in rows:\n",
        "            if not r.get(\"date\"): r[\"date\"] = header.get(\"date\")\n",
        "            if not r.get(\"page_number\"):\n",
        "              r[\"page_number\"] = header.get(\"page_number\") #or page_index  # page_index is the (i+1) you pass to page_to_articles_hybrid\n",
        "            if not r.get(\"publication\"): r[\"publication\"] = header.get(\"publication\")\n",
        "            if not r.get(\"news_section\"): r[\"news_section\"] = header.get(\"news_section\")\n",
        "        all_rows.extend(rows)\n",
        "    df = pd.DataFrame(all_rows)\n",
        "    return df\n",
        "\n",
        "# --------------- Run & Export ---------------\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(os.path.dirname(OUTPUT_CSV_DRIVE_PATH), exist_ok=True)\n",
        "    df = extract_newspaper_metadata(PDF_PATH)\n",
        "    if df.empty:\n",
        "        logger.warning(\"No articles extracted.\")\n",
        "    else:\n",
        "        try:\n",
        "            df.to_csv(OUTPUT_CSV_DRIVE_PATH, index=False)\n",
        "            logger.info(f\"Wrote CSV to {OUTPUT_CSV_DRIVE_PATH}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error writing CSV: {e}\")\n",
        "            df.to_csv(\"newspaper_metadata_v4_fallback.csv\", index=False)\n",
        "            logger.info(\"Wrote fallback CSV locally.\")\n",
        "    print(df.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VpsujHgOZA4",
        "outputId": "56a3ca13-34df-4373-fc66-16b3e0b7a93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         date  page_number        news_section  \\\n",
            "0                        None            1            BIG READ   \n",
            "1                        None            1            BIG READ   \n",
            "2   Monday, September 1, 2025            2                None   \n",
            "3   Monday, September 1, 2025            2                None   \n",
            "4   Monday, September 1, 2025            2                None   \n",
            "5   Monday, September 1, 2025            2                None   \n",
            "6   Monday, September 1, 2025            3  CORRIDORS OF POWER   \n",
            "7   Monday, September 1, 2025            4                NEWS   \n",
            "8   Monday, September 1, 2025            4                NEWS   \n",
            "9                        None            5                None   \n",
            "10                       None            5                None   \n",
            "11                       None            6                NEWS   \n",
            "12                       None            6                NEWS   \n",
            "13                       None            7                NEWS   \n",
            "14                       None            7                NEWS   \n",
            "15                       None            8  BUSINESS PICTORIAL   \n",
            "16                       None            8  BUSINESS PICTORIAL   \n",
            "17                       None            8  BUSINESS PICTORIAL   \n",
            "18                       None            8  BUSINESS PICTORIAL   \n",
            "19                       None            8  BUSINESS PICTORIAL   \n",
            "\n",
            "                                        article_title     byline_raw  \\\n",
            "0   WARM WELCOME: President William Ruto is receiv...     /PCS; /PCS   \n",
            "1   wycliffe muga news story pg 4-5 a distinguishe...  WYCLIFFE MUGA   \n",
            "2   A child receives new typhoid vaccine at Nyalen...   FAITH MATETE   \n",
            "3   enos teche healthcare photo story faith matete...     ENOS TECHE   \n",
            "4   faith matete 1. hinga road at harambee market ...   FAITH MATETE   \n",
            "5   john muchangi @jomunji the researchers said pa...  JOHN MUCHANGI   \n",
            "6   /file cartoon market capitalisation and total ...         / FILE   \n",
            "7   jacktone lawi james ayugi the government took ...  JACKTONE LAWI   \n",
            "8   martin mwita and jacktone lawi james ayugi the...   MARTIN MWITA   \n",
            "9             President William Ruto during the ﬁ rst      /ECITIZEN   \n",
            "10  /ecitizen it was established that equi- ty ban...      /ECITIZEN   \n",
            "11             Lilly  Ajarova on the highest point of       /HANDOUT   \n",
            "12  /handout monday, september 1, 2025 6 on how i ...       /HANDOUT   \n",
            "13  IIA Kenya chairperson, CPA Lilian Mwangi durin...       /HANDOUT   \n",
            "14  /handout martin mwita @mwitamartin chief audit...       /HANDOUT   \n",
            "15  Nagnouma Koné, Global Green Growth Institute m...       /HANDOUT   \n",
            "16  Dr Hussein Endris, ICPAC acting head of Climat...       /HANDOUT   \n",
            "17         Kenya Power’s Dr Jeremiah Kiplagat, Energy       /HANDOUT   \n",
            "18  Nicky Dhanjal, the managing director Traveller...       /HANDOUT   \n",
            "19  Ruby Brooke receives a prize from Java Foundation       /HANDOUT   \n",
            "\n",
            "   reporter_matched reporter_match_scores   article_type  \\\n",
            "0        /PCS; /PCS               100;100          PHOTO   \n",
            "1     WYCLIFFE MUGA                   100  SINGLE-AUTHOR   \n",
            "2      FAITH MATETE                   100          PHOTO   \n",
            "3        ENOS TECHE                   100  SINGLE-AUTHOR   \n",
            "4      FAITH MATETE                   100  SINGLE-AUTHOR   \n",
            "5     JOHN MUCHANGI                   100  SINGLE-AUTHOR   \n",
            "6            / FILE                   100  SINGLE-AUTHOR   \n",
            "7     JACKTONE LAWI                   100  SINGLE-AUTHOR   \n",
            "8      MARTIN MWITA                   100  SINGLE-AUTHOR   \n",
            "9         /ECITIZEN                   100          PHOTO   \n",
            "10        /ECITIZEN                   100  SINGLE-AUTHOR   \n",
            "11         /HANDOUT                   100          PHOTO   \n",
            "12         /HANDOUT                   100  SINGLE-AUTHOR   \n",
            "13         /HANDOUT                   100          PHOTO   \n",
            "14         /HANDOUT                   100  SINGLE-AUTHOR   \n",
            "15         /HANDOUT                   100          PHOTO   \n",
            "16         /HANDOUT                   100          PHOTO   \n",
            "17         /HANDOUT                   100          PHOTO   \n",
            "18         /HANDOUT                   100          PHOTO   \n",
            "19         /HANDOUT                   100          PHOTO   \n",
            "\n",
            "                 photo_credit                    topic  word_count  \\\n",
            "0                         PCS                    other          30   \n",
            "1                         PCS  Health Systems & Policy         177   \n",
            "2                FAITH MATETE                    other          16   \n",
            "3                        None  Health Systems & Policy         540   \n",
            "4                  ENOS TECHE  Health Systems & Policy         564   \n",
            "5                FAITH MATETE  Health Systems & Policy         764   \n",
            "6                        FILE  Health Systems & Policy         113   \n",
            "7   SWEBMASTERS KENYA LIMITED                Education         184   \n",
            "8   SWEBMASTERS KENYA LIMITED                Education         187   \n",
            "9                    ECITIZEN                    other          19   \n",
            "10            ECITIZEN IT WAS                 Business         587   \n",
            "11                    HANDOUT                    other          12   \n",
            "12                    HANDOUT  Health Systems & Policy        1174   \n",
            "13                    HANDOUT                    other          12   \n",
            "14                    HANDOUT  Health Systems & Policy         341   \n",
            "15                    HANDOUT                 Business          36   \n",
            "16                    HANDOUT                 Business          41   \n",
            "17                    HANDOUT                 Business          35   \n",
            "18                    HANDOUT                 Business          37   \n",
            "19                    HANDOUT                    Crime          22   \n",
            "\n",
            "                                              excerpt publication  \n",
            "0   WARM WELCOME: President William Ruto is receiv...    the-star  \n",
            "1   wycliffe muga news story pg 4-5 a distinguishe...    the-star  \n",
            "2   A child receives new typhoid vaccine at Nyalen...    THE-STAR  \n",
            "3   enos teche healthcare photo story faith matete...    THE-STAR  \n",
            "4   faith matete 1. hinga road at harambee market ...    THE-STAR  \n",
            "5   john muchangi @jomunji the researchers said pa...    THE-STAR  \n",
            "6   /file cartoon market capitalisation and total ...    THE-STAR  \n",
            "7   jacktone lawi james ayugi the government took ...    THE-STAR  \n",
            "8   martin mwita and jacktone lawi james ayugi the...    THE-STAR  \n",
            "9   President \\nWilliam Ruto \\nduring the ﬁ rst \\n...    THE-STAR  \n",
            "10  /ecitizen it was established that equi- ty ban...    THE-STAR  \n",
            "11  Lilly  Ajarova \\non the highest \\npoint of \\nU...    THE-STAR  \n",
            "12  /handout monday, september 1, 2025 6 on how i ...    THE-STAR  \n",
            "13  IIA Kenya chairperson, CPA Lilian Mwangi durin...    THE-STAR  \n",
            "14  /handout martin mwita @mwitamartin chief audit...    THE-STAR  \n",
            "15  Nagnouma Koné, Global Green Growth Institute m...        None  \n",
            "16  Dr Hussein Endris, ICPAC acting head of Climat...        None  \n",
            "17  Kenya Power’s \\nDr Jeremiah \\nKiplagat, Energy...        None  \n",
            "18  Nicky Dhanjal, the managing director Traveller...        None  \n",
            "19  Ruby Brooke receives a prize from Java Foundat...        None  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}